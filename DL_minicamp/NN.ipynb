{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40ca666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    def __init__(self, X, y, n_hidden_neurons, output_act_fn='sigmoid', error_fn='cross_entropy'):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_input_neurons = X.shape[1]\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.output_act_fn = output_act_fn\n",
    "        self.error_fn = error_fn\n",
    "\n",
    "        # He initialization for stability\n",
    "        self.input_hidden_weights = np.random.randn(self.n_input_neurons, self.n_hidden_neurons) * np.sqrt(2 / self.n_input_neurons)\n",
    "        self.hidden_biases = np.zeros(self.n_hidden_neurons)\n",
    "        self.hidden_output_weights = np.random.randn(self.n_hidden_neurons, 1) * np.sqrt(2 / self.n_hidden_neurons)\n",
    "        self.output_bias = np.zeros(1)\n",
    "\n",
    "    # ---------------- Activation Functions ----------------\n",
    "    def activation(self, x, act_fn):\n",
    "        if act_fn == 'sigmoid':\n",
    "            x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif act_fn == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif act_fn == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            raise Exception('Unknown activation function')\n",
    "\n",
    "    def activation_derivative(self, x, act_fn):\n",
    "        if act_fn == 'sigmoid':\n",
    "            return x * (1 - x)  # x here is the sigmoid output\n",
    "        elif act_fn == 'relu':\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        elif act_fn == 'linear':\n",
    "            return 1\n",
    "        else:\n",
    "            raise Exception('Unknown activation function')\n",
    "\n",
    "    # ---------------- Forward Propagation ----------------\n",
    "    def forward_pass(self, X):\n",
    "        # Hidden layer\n",
    "        self.z_hidden = np.dot(X, self.input_hidden_weights) + self.hidden_biases\n",
    "        self.hidden = self.activation(self.z_hidden, 'relu')\n",
    "\n",
    "        # Output layer\n",
    "        self.z_output = np.dot(self.hidden, self.hidden_output_weights) + self.output_bias\n",
    "        self.output = self.activation(self.z_output, self.output_act_fn)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    # ---------------- Error Computation ----------------\n",
    "    def error(self, y_true, y_pred):\n",
    "        if self.error_fn == 'mse':\n",
    "            return np.mean(np.square(y_true - y_pred))\n",
    "        elif self.error_fn == 'cross_entropy':\n",
    "            y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
    "            return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        else:\n",
    "            raise Exception('Unknown error function')\n",
    "\n",
    "    def error_derivative(self, y_true, y_pred):\n",
    "        # Simplified stable gradient for cross-entropy\n",
    "        if self.error_fn == 'mse':\n",
    "            return 2 * (y_pred - y_true) / y_true.size\n",
    "        elif self.error_fn == 'cross_entropy':\n",
    "            return (y_pred - y_true) / y_true.size\n",
    "        else:\n",
    "            raise Exception('Unknown error function')\n",
    "\n",
    "    # ---------------- Backward Propagation ----------------\n",
    "    def backward_pass(self, X, y_true, y_pred, learning_rate):\n",
    "        # Output layer\n",
    "        self.output_error = self.error_derivative(y_true, y_pred) * self.activation_derivative(y_pred, self.output_act_fn)\n",
    "        self.hidden_output_weights -= learning_rate * np.dot(self.hidden.T, self.output_error)\n",
    "        self.output_bias -= learning_rate * np.sum(self.output_error, axis=0)\n",
    "\n",
    "        # Hidden layer\n",
    "        self.hidden_error = np.dot(self.output_error, self.hidden_output_weights.T) * self.activation_derivative(self.hidden, 'relu')\n",
    "        self.input_hidden_weights -= learning_rate * np.dot(X.T, self.hidden_error)\n",
    "        self.hidden_biases -= learning_rate * np.sum(self.hidden_error, axis=0)\n",
    "\n",
    "        return self.input_hidden_weights, self.hidden_biases, self.hidden_output_weights, self.output_bias\n",
    "\n",
    "    # ---------------- Training ----------------\n",
    "    def train(self, X, y, learning_rate=0.001, epochs=5000):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward_pass(X)\n",
    "            wih, bh, who, bo = self.backward_pass(X, y, y_pred, learning_rate)\n",
    "            if epoch % 500 == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {self.error(y, y_pred):.4f}')\n",
    "        print('✅ Training complete!')\n",
    "        return wih, bh, who, bo\n",
    "\n",
    "    # ---------------- Prediction ----------------\n",
    "    def predict(self, X):\n",
    "        preds = self.forward_pass(X)\n",
    "        if self.error_fn == 'mse':\n",
    "            return preds\n",
    "        elif self.error_fn == 'cross_entropy':\n",
    "            return np.where(preds > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232c15fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 17433.7012\n",
      "Epoch: 500, Loss: 3.8558\n",
      "Epoch: 1000, Loss: 1.8428\n",
      "Epoch: 1500, Loss: 1.2564\n",
      "Epoch: 2000, Loss: 0.9450\n",
      "Epoch: 2500, Loss: 0.7630\n",
      "Epoch: 3000, Loss: 0.6403\n",
      "Epoch: 3500, Loss: 0.5501\n",
      "Epoch: 4000, Loss: 0.4783\n",
      "Epoch: 4500, Loss: 0.4242\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1 , random_state=42)\n",
    "y = y.reshape(-1, 1)  # Reshape y to be a 2D array\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Initialize and train the neural network\n",
    "nn = NeuralNetwork(X_train, y_train, n_hidden_neurons=128, output_act_fn='linear', error_fn='mse')\n",
    "wih , bh , who , bo = nn.train(X_train, y_train, learning_rate=0.001, epochs=5000 )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579383d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9999\n",
      "Mean Squared Error: 2.2291\n",
      "Actual: 42.67, Predicted: 43.69\n",
      "Actual: 75.01, Predicted: 73.48\n",
      "Actual: -4.06, Predicted: -4.73\n",
      "Actual: -295.72, Predicted: -296.71\n",
      "Actual: 44.43, Predicted: 44.72\n",
      "Actual: 21.68, Predicted: 20.70\n",
      "Actual: -146.52, Predicted: -145.94\n",
      "Actual: -60.56, Predicted: -59.54\n",
      "Actual: -15.32, Predicted: -13.44\n",
      "Actual: 176.65, Predicted: 178.07\n",
      "Actual: -325.84, Predicted: -325.62\n",
      "Actual: -56.63, Predicted: -57.26\n",
      "Actual: 46.26, Predicted: 45.16\n",
      "Actual: -88.99, Predicted: -89.48\n",
      "Actual: 127.20, Predicted: 126.43\n",
      "Actual: -134.09, Predicted: -132.58\n",
      "Actual: -63.02, Predicted: -62.61\n",
      "Actual: 47.02, Predicted: 47.18\n",
      "Actual: 92.68, Predicted: 94.89\n",
      "Actual: -40.77, Predicted: -41.47\n",
      "Actual: 128.26, Predicted: 127.64\n",
      "Actual: 239.29, Predicted: 237.24\n",
      "Actual: 191.73, Predicted: 193.44\n",
      "Actual: -129.84, Predicted: -130.68\n",
      "Actual: 184.19, Predicted: 183.33\n",
      "Actual: -123.97, Predicted: -125.10\n",
      "Actual: -247.00, Predicted: -247.02\n",
      "Actual: 250.06, Predicted: 249.37\n",
      "Actual: 41.54, Predicted: 42.68\n",
      "Actual: 225.90, Predicted: 226.21\n",
      "Actual: 221.54, Predicted: 221.93\n",
      "Actual: -134.98, Predicted: -135.03\n",
      "Actual: 37.30, Predicted: 36.72\n",
      "Actual: 70.76, Predicted: 70.84\n",
      "Actual: -52.45, Predicted: -54.91\n",
      "Actual: -183.72, Predicted: -184.16\n",
      "Actual: 44.97, Predicted: 47.20\n",
      "Actual: -36.52, Predicted: -34.44\n",
      "Actual: 26.66, Predicted: 27.68\n",
      "Actual: -101.40, Predicted: -100.35\n",
      "Actual: 57.08, Predicted: 56.42\n",
      "Actual: -26.15, Predicted: -24.57\n",
      "Actual: 25.78, Predicted: 26.38\n",
      "Actual: 89.64, Predicted: 90.11\n",
      "Actual: -81.22, Predicted: -78.65\n",
      "Actual: -31.11, Predicted: -33.76\n",
      "Actual: -82.02, Predicted: -82.24\n",
      "Actual: -152.02, Predicted: -152.92\n",
      "Actual: 168.68, Predicted: 170.26\n",
      "Actual: -124.85, Predicted: -123.04\n",
      "Actual: -35.40, Predicted: -34.33\n",
      "Actual: 188.86, Predicted: 188.01\n",
      "Actual: 286.98, Predicted: 285.58\n",
      "Actual: -80.77, Predicted: -81.40\n",
      "Actual: -41.80, Predicted: -39.31\n",
      "Actual: -98.71, Predicted: -99.63\n",
      "Actual: 156.00, Predicted: 153.93\n",
      "Actual: 62.47, Predicted: 62.00\n",
      "Actual: 81.64, Predicted: 81.89\n",
      "Actual: -14.57, Predicted: -11.23\n",
      "Actual: -160.09, Predicted: -162.15\n",
      "Actual: 11.64, Predicted: 13.27\n",
      "Actual: -49.86, Predicted: -52.60\n",
      "Actual: -50.99, Predicted: -54.09\n",
      "Actual: 141.58, Predicted: 142.17\n",
      "Actual: 183.39, Predicted: 182.25\n",
      "Actual: 215.29, Predicted: 213.46\n",
      "Actual: 61.27, Predicted: 62.47\n",
      "Actual: 145.23, Predicted: 144.51\n",
      "Actual: 72.53, Predicted: 71.46\n",
      "Actual: 31.71, Predicted: 30.93\n",
      "Actual: 10.14, Predicted: 13.20\n",
      "Actual: 61.09, Predicted: 60.89\n",
      "Actual: -100.84, Predicted: -100.66\n",
      "Actual: -162.42, Predicted: -160.60\n",
      "Actual: -73.73, Predicted: -75.43\n",
      "Actual: 31.48, Predicted: 32.37\n",
      "Actual: -11.50, Predicted: -7.89\n",
      "Actual: 171.84, Predicted: 172.56\n",
      "Actual: -38.91, Predicted: -37.30\n",
      "Actual: 39.39, Predicted: 39.53\n",
      "Actual: 20.20, Predicted: 20.66\n",
      "Actual: -202.49, Predicted: -201.59\n",
      "Actual: -92.82, Predicted: -92.48\n",
      "Actual: 6.57, Predicted: 7.28\n",
      "Actual: -173.68, Predicted: -173.15\n",
      "Actual: -13.18, Predicted: -14.00\n",
      "Actual: 342.34, Predicted: 341.77\n",
      "Actual: -51.49, Predicted: -53.24\n",
      "Actual: -107.69, Predicted: -106.73\n",
      "Actual: 129.94, Predicted: 129.90\n",
      "Actual: 37.88, Predicted: 39.85\n",
      "Actual: 85.61, Predicted: 85.51\n",
      "Actual: 129.39, Predicted: 129.64\n",
      "Actual: 158.61, Predicted: 157.82\n",
      "Actual: 258.55, Predicted: 259.62\n",
      "Actual: 227.29, Predicted: 227.38\n",
      "Actual: -245.45, Predicted: -245.45\n",
      "Actual: -169.82, Predicted: -169.65\n",
      "Actual: -5.06, Predicted: -5.62\n",
      "Actual: 200.58, Predicted: 200.95\n",
      "Actual: -41.32, Predicted: -42.98\n",
      "Actual: 63.86, Predicted: 65.12\n",
      "Actual: -44.14, Predicted: -42.31\n",
      "Actual: -20.82, Predicted: -23.73\n",
      "Actual: -20.28, Predicted: -20.26\n",
      "Actual: -143.53, Predicted: -145.61\n",
      "Actual: -81.09, Predicted: -79.76\n",
      "Actual: -13.19, Predicted: -10.83\n",
      "Actual: 35.47, Predicted: 35.37\n",
      "Actual: 5.31, Predicted: 4.50\n",
      "Actual: -60.98, Predicted: -59.15\n",
      "Actual: 25.97, Predicted: 29.15\n",
      "Actual: 111.13, Predicted: 111.87\n",
      "Actual: 54.19, Predicted: 55.97\n",
      "Actual: 8.67, Predicted: 8.04\n",
      "Actual: 243.72, Predicted: 243.50\n",
      "Actual: -4.57, Predicted: -4.20\n",
      "Actual: 121.54, Predicted: 122.00\n",
      "Actual: 225.39, Predicted: 225.45\n",
      "Actual: -211.85, Predicted: -212.07\n",
      "Actual: -116.58, Predicted: -118.09\n",
      "Actual: 169.02, Predicted: 168.32\n",
      "Actual: 132.63, Predicted: 130.56\n",
      "Actual: -16.33, Predicted: -16.24\n",
      "Actual: -100.54, Predicted: -98.92\n",
      "Actual: -49.68, Predicted: -47.81\n",
      "Actual: 78.01, Predicted: 75.70\n",
      "Actual: 44.94, Predicted: 42.23\n",
      "Actual: -32.50, Predicted: -30.88\n",
      "Actual: 169.79, Predicted: 168.35\n",
      "Actual: 42.34, Predicted: 39.74\n",
      "Actual: 97.52, Predicted: 99.78\n",
      "Actual: 106.61, Predicted: 107.62\n",
      "Actual: 82.79, Predicted: 78.57\n",
      "Actual: -75.56, Predicted: -76.86\n",
      "Actual: 138.33, Predicted: 138.41\n",
      "Actual: -122.04, Predicted: -121.00\n",
      "Actual: 154.95, Predicted: 155.15\n",
      "Actual: -142.57, Predicted: -143.41\n",
      "Actual: 195.33, Predicted: 194.79\n",
      "Actual: -42.95, Predicted: -46.86\n",
      "Actual: -80.77, Predicted: -80.97\n",
      "Actual: -21.84, Predicted: -22.57\n",
      "Actual: 200.54, Predicted: 200.75\n",
      "Actual: -27.55, Predicted: -26.57\n",
      "Actual: -69.01, Predicted: -71.49\n",
      "Actual: -2.71, Predicted: -4.51\n",
      "Actual: 32.91, Predicted: 31.61\n",
      "Actual: 145.78, Predicted: 147.57\n",
      "Actual: 123.35, Predicted: 123.61\n",
      "Actual: -65.97, Predicted: -72.61\n",
      "Actual: -27.27, Predicted: -28.31\n",
      "Actual: -30.62, Predicted: -35.22\n",
      "Actual: 46.16, Predicted: 46.81\n",
      "Actual: -86.91, Predicted: -86.60\n",
      "Actual: 157.46, Predicted: 156.62\n",
      "Actual: 5.80, Predicted: 8.63\n",
      "Actual: -69.44, Predicted: -69.70\n",
      "Actual: -19.36, Predicted: -18.44\n",
      "Actual: -326.15, Predicted: -325.67\n",
      "Actual: 201.76, Predicted: 202.02\n",
      "Actual: 106.77, Predicted: 105.43\n",
      "Actual: 34.34, Predicted: 35.67\n",
      "Actual: 199.38, Predicted: 200.59\n",
      "Actual: -209.91, Predicted: -208.50\n",
      "Actual: -126.74, Predicted: -127.96\n",
      "Actual: -0.39, Predicted: 0.67\n",
      "Actual: 213.58, Predicted: 214.14\n",
      "Actual: 22.38, Predicted: 24.06\n",
      "Actual: 98.31, Predicted: 98.29\n",
      "Actual: 24.53, Predicted: 22.95\n",
      "Actual: -159.43, Predicted: -159.42\n",
      "Actual: 124.65, Predicted: 123.26\n",
      "Actual: 126.76, Predicted: 126.90\n",
      "Actual: 163.73, Predicted: 162.69\n",
      "Actual: -63.40, Predicted: -63.12\n",
      "Actual: 159.01, Predicted: 160.00\n",
      "Actual: 28.12, Predicted: 27.94\n",
      "Actual: 96.72, Predicted: 98.34\n",
      "Actual: -225.22, Predicted: -226.07\n",
      "Actual: -30.60, Predicted: -31.93\n",
      "Actual: 216.71, Predicted: 215.75\n",
      "Actual: -168.33, Predicted: -168.89\n",
      "Actual: -325.99, Predicted: -325.82\n",
      "Actual: 133.34, Predicted: 133.70\n",
      "Actual: 108.93, Predicted: 109.36\n",
      "Actual: 204.04, Predicted: 205.19\n",
      "Actual: -25.82, Predicted: -27.58\n",
      "Actual: 66.44, Predicted: 64.63\n",
      "Actual: 230.86, Predicted: 229.60\n",
      "Actual: 29.63, Predicted: 30.12\n",
      "Actual: 180.11, Predicted: 181.40\n",
      "Actual: -23.82, Predicted: -24.42\n",
      "Actual: 195.22, Predicted: 194.69\n",
      "Actual: -275.10, Predicted: -273.54\n",
      "Actual: -79.70, Predicted: -80.58\n",
      "Actual: -94.70, Predicted: -96.07\n",
      "Actual: -42.84, Predicted: -43.70\n",
      "Actual: 81.53, Predicted: 81.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "# Make predictions on the test set  \n",
    "y_pred = nn.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(f'R^2 Score: {r2_score(y_test, y_pred):.4f}')\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred):.4f}')\n",
    "# ---------------- Evaluation ----------------\n",
    "for i in range(len(y_test)):\n",
    "    print(f'Actual: {y_test[i][0]:.2f}, Predicted: {y_pred[i][0]:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a131cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R^2 Score: 1.0000\n",
      "Linear Regression Mean Squared Error: 0.0095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Train a linear regression model for comparison\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_lr_pred = lr.predict(X_test)\n",
    "# Evaluate the linear regression model\n",
    "print(f'Linear Regression R^2 Score: {r2_score(y_test, y_lr_pred):.4f}')\n",
    "print(f'Linear Regression Mean Squared Error: {mean_squared_error(y_test, y_lr_pred):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d1b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Regressor R^2 Score: 0.9999\n",
      "MLP Regressor Mean Squared Error: 2.1758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# Train a built-in MLP regressor for comparison\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(128,), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "mlp.fit(X_train, y_train.ravel())\n",
    "y_mlp_pred = mlp.predict(X_test)\n",
    "# Evaluate the MLP regressor    \n",
    "print(f'MLP Regressor R^2 Score: {r2_score(y_test, y_mlp_pred):.4f}')\n",
    "print(f'MLP Regressor Mean Squared Error: {mean_squared_error(y_test, y_mlp_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aadd28b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8644\n",
      "Epoch: 500, Loss: 0.6629\n",
      "Epoch: 1000, Loss: 0.5848\n",
      "Epoch: 1500, Loss: 0.5432\n",
      "Epoch: 2000, Loss: 0.5141\n",
      "Epoch: 2500, Loss: 0.4915\n",
      "Epoch: 3000, Loss: 0.4734\n",
      "Epoch: 3500, Loss: 0.4585\n",
      "Epoch: 4000, Loss: 0.4461\n",
      "Epoch: 4500, Loss: 0.4356\n",
      "✅ Training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "# Generate a classification dataset\n",
    "X_clf, y_clf = make_classification(n_samples=1000, n_features=10, n_classes=2, n_informative=5, random_state=42)\n",
    "y_clf = y_clf.reshape(-1, 1)  # Reshape y to be a 2D array\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "X_clf_train = scaler.fit_transform(X_clf_train)\n",
    "X_clf_test = scaler.transform(X_clf_test)\n",
    "# Initialize and train the neural network for classification\n",
    "nn_clf = NeuralNetwork(X_clf_train, y_clf_train, n_hidden_neurons=128, output_act_fn='sigmoid', error_fn='cross_entropy')\n",
    "wih_clf , bh_clf , who_clf , bo_clf = nn_clf.train(X_clf_train, y_clf_train, learning_rate=0.001, epochs=5000 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4511bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Classification Results:\n",
      "Accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Network Classification Results:\" )\n",
    "y_clf_pred = nn_clf.predict(X_clf_test)\n",
    "accuracy = np.mean(y_clf_pred == y_clf_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599c5cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier Accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# Train a built-in MLP classifier for comparison\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver ='adam', max_iter=5000, random_state=42)\n",
    "mlp_clf.fit(X_clf_train, y_clf_train.ravel())\n",
    "y_mlp_clf_pred = mlp_clf.predict(X_clf_test)\n",
    "mlp_accuracy = np.mean(y_mlp_clf_pred == y_clf_test.ravel())\n",
    "print(f'MLP Classifier Accuracy: {mlp_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
